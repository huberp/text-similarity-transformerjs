# text-similarity-transformerjs

A GitHub repository demonstrating text similarity analysis using Transformer.js. This project uses embeddings from transformer models to detect similarities between documents in a test corpus.

## Overview

This repository contains:
- **Test Corpus**: 25 markdown documents across 3 topics (Math, Fruit, LLM)
- **Corpus Index**: A table listing all documents with their topics and subtopics
- **Similarity Script**: Node.js script using Transformer.js to compute document similarities
- **GitHub Workflow**: Automated workflow that runs similarity analysis

## Test Corpus

The test corpus contains 25 short text documents (max 400 words each) organized by topic:

- **LLM** (8 documents): Documents about Large Language Models covering various subtopics like architecture, prompting, tokenization, RAG, evaluation, constraints, learning paradigms, and safety
- **Math** (8 documents): Mathematical topics including linear algebra, calculus, probability, number theory, graph theory, statistics, geometry, and discrete mathematics  
- **Fruit** (9 documents): Various fruit categories including citrus, berries, tropical fruits, stone fruits, apples, melons, grapes, exotic varieties, and pomegranates

See [corpus_index.md](corpus_index.md) for the complete list with topics and subtopics.

## How It Works

The repository provides two complementary text analysis approaches:

### 1. Text Similarity Analysis (Transformer-based)
The similarity analysis uses:
- **Transformer.js** (`@xenova/transformers`) - Run transformer models in Node.js
- **Sentence Embeddings** - Uses the `bge-base-en-v1.5` model with 8-bit quantization (q8) to generate embeddings, providing a good balance between model size/performance and accuracy
- **Cosine Similarity** - Computes similarity scores between document embeddings

The script analyzes:
- Top most similar document pairs
- Average similarity within each topic
- Average similarity across different topics

### 2. TF-IDF Analysis (Classic NLP)
The TF-IDF analysis uses:
- **tiny-tfidf** - Minimal implementation of Term Frequency-Inverse Document Frequency
- **BM25 weighting** - Advanced term weighting scheme for better relevance scoring
- **Stopword filtering** - Removes common words to focus on distinctive terms

The script computes:
- Term frequency (TF) for each term in each document
- Inverse document frequency (IDF) for each term across the corpus
- Top distinctive terms for each document based on TF-IDF scores

## Installation

```bash
npm install
```

## Usage

### Text Similarity Analysis

Run the similarity analysis locally:

```bash
npm run similarity
```

The script will:
1. Load the embedding model
2. Read all documents from the test_corpus directory
3. Generate embeddings for each document
4. Calculate similarity scores between all document pairs
5. Display analysis results including top similar pairs and topic-based statistics
6. Export two CSV files:
   - **embeddings.csv**: Contains filename, topic, subtopic, and all embedding dimensions for each document
   - **similarity_results.csv**: Contains detailed similarity scores between all document pairs

### TF-IDF Analysis

Run the TF-IDF analysis locally:

```bash
npm run tfidf
```

The script will:
1. Read all documents from the test_corpus directory
2. Compute TF-IDF scores using the tiny-tfidf library
3. Display top terms for each document
4. Export six CSV files:
   - **tf.csv**: Contains term frequency matrix with documents as rows and terms as columns (wide format)
   - **tf_sparse.csv**: Contains term frequency in sparse format (document_id, term_id, frequency) - more efficient for large corpora
   - **idf.csv**: Contains inverse document frequency scores for all terms in the corpus (sorted by idf_weight)
   - **document_index.csv**: Maps document IDs to document filenames
   - **term_index.csv**: Maps term IDs to terms (sorted by idf_weight)
   - **term_documents.csv**: Inverted index showing which documents contain each term

### TF-IDF Vector Store

Build a vector store using normalized TF-IDF vectors:

```bash
npm run tfidf-vectors
```

The script will:
1. Read TF-IDF data from the tfidf-data directory (generated by `npm run tfidf`)
2. Build TF-IDF vectors for each document (one vector per document, indexed by term ID)
3. Normalize each vector to unit size using L2 normalization
4. Create a LocalIndex vector store in the tfidf-vector-index directory
5. Export normalized vectors to tfidf_vectors.csv for analysis

This creates a second vector store that can be used for similarity queries based on TF-IDF rather than transformer embeddings. The normalized vectors can be compared with the transformer-based embeddings to understand the differences between classical NLP and deep learning approaches.

### Output Files

The scripts generate CSV files that can be used for further analysis:

**embeddings.csv** (from similarity analysis)
- Contains one row per document with columns: `filename`, `topic`, `subtopic`, `dim_0`, `dim_1`, ..., `dim_N`
- The embedding dimensions can be loaded into Jupyter/Pandas for analysis and visualization
- Example: 25 documents with 768-dimensional embeddings

**similarity_results.csv** (from similarity analysis)
- Contains similarity scores between all document pairs
- Columns: `document1_filename`, `document1_topic`, `document1_subtopic`, `document2_filename`, `document2_topic`, `document2_subtopic`, `similarity_score`, `same_topic`
- Sorted by similarity score (highest to lowest)
- Can be used for detailed similarity analysis and filtering

**tf.csv** (from TF-IDF analysis)
- Contains term frequency matrix in wide format
- Columns: `document`, `topic`, `subtopic`, followed by all unique terms in the corpus
- Each cell contains the raw frequency count of a term in a document
- Example: 25 documents × 1643 unique terms
- Note: This format is not recommended for large corpora (use tf_sparse.csv instead)

**tf_sparse.csv** (from TF-IDF analysis)
- Contains term frequency in sparse format for efficient storage
- Columns: `document_id`, `term_id`, `frequency`
- Only includes entries where a term appears in a document (frequency > 0)
- More efficient for large corpora as it avoids storing zeros
- Use with document_index.csv and term_index.csv to look up actual document names and terms

**idf.csv** (from TF-IDF analysis)
- Contains inverse document frequency scores
- Columns: `term`, `idf_weight`, `collection_frequency`
- Sorted by `idf_weight` in descending order (most distinctive terms first)
- Each row represents a unique term with its IDF weight and the number of documents containing it
- Higher IDF weights indicate more distinctive/rare terms

**document_index.csv** (from TF-IDF analysis)
- Maps numeric document IDs to document filenames
- Columns: `document_id`, `document`, `topic`, `subtopic`
- Used to look up document names from IDs in tf_sparse.csv and term_documents.csv

**term_index.csv** (from TF-IDF analysis)
- Maps numeric term IDs to terms
- Columns: `term_id`, `term`, `idf_weight`, `collection_frequency`
- Sorted by `idf_weight` in descending order (same order as idf.csv)
- Used to look up term names from IDs in tf_sparse.csv

**term_documents.csv** (from TF-IDF analysis)
- Inverted index showing which documents contain each term
- Columns: `term_id`, `term`, `document_ids`
- The `document_ids` column contains a semicolon-separated list of document IDs
- Useful for finding all documents containing a specific term
- Example: For term "fruits", document_ids might be "0;1;2;3;4;5;6;7;8"

**tfidf_vectors.csv** (from TF-IDF vector store)
- Contains normalized TF-IDF vectors for each document
- Columns: `filename`, `topic`, `subtopic`, `term_0`, `term_1`, ..., `term_N`
- Each term column corresponds to a term ID from term_index.csv
- Vector values are normalized to unit length using L2 normalization (magnitude = 1.0)
- Can be used for similarity analysis and comparison with transformer embeddings
- Example: 25 documents × 1265 term dimensions (sparse vectors with many zeros)


See [OUTPUT_EXAMPLES.md](OUTPUT_EXAMPLES.md) for detailed examples and usage patterns for these CSV files.

## GitHub Workflows

The repository includes four GitHub Actions workflows:

### CI Workflow (`.github/workflows/ci.yml`)

The CI workflow runs on all pushes and pull requests and includes:

1. **Linting**: Runs ESLint to check code quality
2. **Security Check** (PR only): 
   - Detects when dependencies are added or updated in pull requests
   - Automatically triggered for Dependabot PRs or any PR that modifies `package.json`/`package-lock.json`
   - Runs security audit using npm audit (GitHub Advisory Database)
   - Posts a comment on the PR with security findings and risk assessment
3. **Conditional Similarity Analysis** (PR only):
   - Only runs when PR modifies `similarity.js` or `.github/workflows/text-similarity.yml`
   - Ensures the similarity script still works correctly after changes
   - Uploads the output CSV files as workflow artifacts with 90-day retention

### Text Similarity Workflow (`.github/workflows/text-similarity.yml`)

The text similarity workflow automatically runs the similarity analysis when:
- Relevant files are modified (`similarity.js`, workflow file, `test_corpus/**`, `package.json`)
- Manually triggered (workflow_dispatch)

The workflow:
- Installs dependencies
- Runs the text similarity analysis script
- Generates embeddings and similarity results
- Uploads the output CSV files (`embeddings.csv` and `similarity_results.csv`) as workflow artifacts with 90-day retention

### TF-IDF Workflow (`.github/workflows/tfidf.yml`)

The TF-IDF workflow automatically runs the TF-IDF analysis using tiny-tfidf when:
- Relevant files are modified (`tfidf.js`, workflow file, `test_corpus/**`, `package.json`)
- Manually triggered (workflow_dispatch)

The workflow:
- Installs dependencies (including tiny-tfidf)
- Runs the TF-IDF analysis script
- Computes term frequency (TF) and inverse document frequency (IDF) for the corpus
- Uploads the output CSV files (`tf.csv`, `tf_sparse.csv`, `idf.csv`, `document_index.csv`, `term_index.csv`, and `term_documents.csv`) as workflow artifacts with 90-day retention

### TF-IDF Vector Store Workflow (`.github/workflows/tfidf-vectors.yml`)

The TF-IDF vector store workflow builds normalized TF-IDF vectors and a LocalIndex when:
- TF-IDF data is modified (`tfidf-data/**`)
- The tfidf-vectors.js script is modified
- The workflow file itself is modified
- Manually triggered (workflow_dispatch)

The workflow:
- Installs dependencies
- Runs the TF-IDF vector store builder
- Creates normalized TF-IDF vectors from the TF-IDF data
- Builds a LocalIndex for similarity queries
- Uploads the vector store (`tfidf-vector-index/`) and normalized vectors CSV (`tfidf_vectors.csv`) as workflow artifacts with 90-day retention

**Accessing Workflow Artifacts**: After the workflow runs, you can download the generated CSV files from the workflow run page under the "Artifacts" section.

## Project Structure

```
.
├── test_corpus/           # Test documents organized by topic
│   ├── llm_01.md         # LLM documents
│   ├── math_01.md        # Math documents
│   └── fruit_01.md       # Fruit documents
├── tfidf-data/           # TF-IDF analysis outputs
├── corpus_index.md        # Table of all documents
├── similarity.js          # Main similarity analysis script
├── tfidf.js              # TF-IDF analysis script
├── tfidf-vectors.js      # TF-IDF vector store builder
├── package.json          # Node.js dependencies
└── .github/
    └── workflows/
        ├── ci.yml             # CI workflow (linting, security, conditional checks)
        ├── text-similarity.yml  # Text similarity analysis workflow
        ├── tfidf.yml            # TF-IDF analysis workflow
        └── tfidf-vectors.yml    # TF-IDF vector store workflow
```

## Requirements

- Node.js 20 or higher
- npm

## Contributing

This project uses the following labels to categorize issues and pull requests:

- **bug** - Something isn't working correctly
- **enhancement** - New feature or improvement request
- **documentation** - Improvements or additions to documentation
- **good first issue** - Good for newcomers to the project
- **help wanted** - Extra attention is needed
- **question** - Further information is requested
- **dependencies** - Updates to project dependencies

## License

MIT
